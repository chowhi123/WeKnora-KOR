## 소개
WeKnora는 지능형 문서 이해 및 검색 기능을 구현하여 프로덕션 환경에 즉시 투입 가능한 기업급 RAG 프레임워크입니다. 이 시스템은 모듈식 설계를 채택하여 문서 이해, 벡터 저장소, 추론 파일 등의 기능을 분리했습니다.

![arc](./images/arc.png)

---

## 파이프라인 (PipeLine)
WeKnora의 문서 처리는 여러 단계를 거칩니다: 삽입 -> 지식 추출 -> 인덱싱 -> 검색 -> 생성. 전체 프로세스는 다양한 검색 방법을 지원합니다.

![](./images/pipeline2.jpeg)


사용자가 업로드한 숙박 명세서 PDF 파일을 예로 들어 데이터 흐름을 자세히 소개하겠습니다.

### 1. 요청 수신 및 초기화
+ **요청 식별**: 시스템은 요청을 수신하고 전체 처리 과정을 추적하기 위해 고유한 `request_id=Lkq0OGLYu2fV`를 할당합니다.
+ **테넌트 및 세션 확인**:
    - 시스템은 먼저 테넌트 정보를 확인합니다 (ID: 1, Name: Default Tenant).
    - 이어서 세션 `1f241340-ae75-40a5-8731-9a3a82e34fdd`에 속한 지식 베이스 질의응답(Knowledge QA) 요청 처리를 시작합니다.
+ **사용자 질문**: 사용자의 원래 질문은 "**투숙한 방 타입은 무엇인가**"입니다.
+ **메시지 생성**: 시스템은 사용자의 질문과 생성될 답변에 대해 각각 메시지 기록을 생성합니다. ID는 각각 `703ddf09-...` 및 `6f057649-...`입니다.

### 2. 지식 베이스 질의응답 프로세스 시작
시스템은 지식 베이스 질의응답 서비스를 정식으로 호출하고, 다음 9개 이벤트를 포함하여 순차적으로 실행될 전체 처리 파이프라인(Pipeline)을 정의합니다:
`[rewrite_query, preprocess_query, chunk_search, chunk_rerank, chunk_merge, filter_top_k, into_chat_message, chat_completion_stream, stream_filter]`

---

### 3. 이벤트 실행 세부 정보
#### 이벤트 1: `rewrite_query` - 질문 재작성
+ **목적**: 검색을 더 정확하게 하기 위해 시스템은 문맥을 결합하여 사용자의 실제 의도를 이해해야 합니다.
+ **작업**:
    1. 시스템은 현재 세션의 최근 메시지 20개를 검색하여(실제로는 8개 검색됨) 문맥으로 사용합니다.
    2. `deepseek-r1:7b`라는 로컬 대규모 언어 모델을 호출합니다.
    3. 모델은 채팅 기록을 분석하여 질문자가 "Liwx"임을 파악하고 원래 질문인 "투숙한 방 타입은 무엇인가"를 더 구체적으로 재작성합니다.
+ **결과**: 질문이 성공적으로 재작성되었습니다: "**Liwx가 이번에 투숙한 방 타입은 무엇인가**".

#### 이벤트 2: `preprocess_query` - 질문 전처리
+ **목적**: 재작성된 질문을 분동(Tokenize)하여 검색 엔진 처리에 적합한 키워드 시퀀스로 변환합니다.
+ **작업**: 재작성된 질문에 대해 분동 처리를 수행했습니다.
+ **결과**: 키워드 문자열이 생성되었습니다: "`필요 재작성 사용자 질문 투숙 방타입 근거 제공 정보 투숙 인 Liwx 선택 방타입 트윈 룸 따라서 재작성 후 전체 질문 위함 Liwx 이번 투숙 방타입`". (역주: 예시 키워드는 한국어 형태소 분석 결과에 따라 다를 수 있음)

#### 이벤트 3: `chunk_search` - 지식 청크 검색
이것은 가장 핵심적인 **검색(Retrieval)** 단계로, 시스템은 두 번의 하이브리드 검색(Hybrid Search)을 수행했습니다.

+ **첫 번째 검색 (재작성된 전체 문장 사용)**:
    - **벡터 검색**:
        1. 임베딩 모델 `bge-m3:latest`를 로드하여 문장을 1024차원 벡터로 변환합니다.
        2. PostgreSQL 데이터베이스에서 벡터 유사도 검색을 수행하여 2개의 관련 지식 청크(chunk)를 찾았습니다. ID는 각각 `e3bf6599-...` 및 `3989c6ce-...`입니다.
    - **키워드 검색**:
        1. 동시에 시스템은 키워드 검색도 수행했습니다.
        2. 마찬가지로 위의 2개 지식 청크를 찾았습니다.
    - **결과 병합**: 두 가지 방법으로 찾은 4개의 결과(실제로는 2개의 중복)가 중복 제거되어 최종적으로 2개의 고유한 지식 청크를 얻었습니다.
+ **두 번째 검색 (전처리된 키워드 시퀀스 사용)**:
    - 시스템은 분동된 키워드를 사용하여 위의 **벡터 검색** 및 **키워드 검색** 과정을 반복했습니다.
    - 최종적으로 동일한 2개의 지식 청크를 얻었습니다.
+ **최종 결과**: 두 번의 검색과 결과 병합을 거쳐 시스템은 가장 관련성이 높은 2개의 지식 청크를 확정하고, 답변 생성을 위해 내용을 추출했습니다.

#### 이벤트 4: `chunk_rerank` - 결과 재정렬
+ **목적**: 더 강력한 모델을 사용하여 1차 검색된 결과를 더 정밀하게 정렬함으로써 최종 답변의 품질을 향상시킵니다.
+ **작업**: 로그에 `Rerank model ID is empty, skipping reranking`이 표시됩니다. 이는 시스템에 재정렬 단계가 구성되었지만 구체적인 재정렬 모델이 지정되지 않아 **이 단계가 건너뛰어졌음**을 의미합니다.

#### 이벤트 5: `chunk_merge` - 청크 병합
+ **목적**: 내용상 인접하거나 관련된 지식 청크를 병합하여 더 완전한 문맥을 형성합니다.
+ **작업**: 시스템은 검색된 2개의 청크를 분석하고 병합을 시도했습니다. 로그에 따르면 최종 처리 후에도 여전히 2개의 독립된 청크였지만 관련성 점수에 따라 정렬되었습니다.

#### 이벤트 6: `filter_top_k` - Top-K 필터링
+ **목적**: 가장 관련성이 높은 K개의 결과만 유지하여 불필요한 정보가 언어 모델을 방해하지 않도록 합니다.
+ **작업**: 시스템은 상위 5개(Top-K = 5)의 가장 관련성 높은 청크를 유지하도록 구성되었습니다. 현재 청크가 2개뿐이므로 모두 이 필터를 통과했습니다.

#### 이벤트 7 & 8: `into_chat_message` & `chat_completion_stream` - 답변 생성
이것은 **생성(Generation)** 단계입니다.

+ **목적**: 검색된 정보를 기반으로 자연스럽고 매끄러운 답변을 생성합니다.
+ **작업**:
    1. 시스템은 검색된 2개의 지식 청크 내용, 사용자의 원래 질문, 채팅 기록을 통합하여 완전한 프롬프트(Prompt)를 형성합니다.
    2. 다시 `deepseek-r1:7b` 대규모 언어 모델을 호출하고 **스트리밍(Stream)** 방식으로 답변 생성을 요청합니다. 스트리밍 출력은 타자기 효과를 구현하여 사용자 경험을 향상시킬 수 있습니다.

#### 이벤트 9: `stream_filter` - 스트리밍 출력 필터링
+ **목적**: 모델이 생성하는 실시간 텍스트 스트림을 후처리하여 불필요한 특수 태그나 내용을 필터링합니다.
+ **작업**:
    - 시스템은 모델이 생각하는 과정에서 생성할 수 있는 `<think>` 및 `</think>`와 같은 내부 태그를 제거하기 위한 필터를 설정했습니다.
    - 로그에 따르면 모델이 출력한 첫 번째 청크는 `<think> 근거`였으며, 필터가 `<think>` 태그를 성공적으로 차단 및 제거하고 "근거" 및 이후 내용만 전달했습니다.

### 4. 완료 및 응답
+ **인용 전송**: 답변 생성과 동시에 시스템은 근거가 된 2개의 지식 청크를 "참고 내용"으로 프론트엔드에 전송하여 사용자가 출처를 확인할 수 있도록 합니다.
+ **메시지 업데이트**: 모델이 모든 내용을 생성하면 시스템은 전체 답변을 이전에 생성된 메시지 기록(ID: `6f057649-...`)에 업데이트합니다.
+ **요청 종료**: 서버는 `200` 성공 상태 코드를 반환하여 질문부터 답변까지의 전체 프로세스가 종료되었음을 알립니다.

### 요약
이 로그는 전형적인 RAG 프로세스를 완전히 기록하고 있습니다: 시스템은 **질문 재작성** 및 **전처리**를 통해 사용자 의도를 정확하게 이해하고, **벡터 및 키워드 하이브리드 검색**을 사용하여 지식 베이스에서 관련 정보를 찾았습니다. **재정렬**은 건너뛰었지만 **병합** 및 **필터링**을 실행했으며, 마지막으로 검색된 지식을 문맥으로 대규모 언어 모델에 전달하여 유창하고 정확한 답변을 **생성**하고 **스트리밍 필터링**을 통해 출력의 순수성을 보장했습니다.

## 문서 파싱 및 분할
코드는 문서 내용의 심층 파싱, 분할 및 멀티모달 정보 추출을 전담하는 독립적이고 gRPC 통신을 사용하는 마이크로서비스를 구현했습니다. 이것이 바로 "비동기 처리" 단계의 핵심 실행자입니다.

### **전체 아키텍처**
이것은 Python 기반의 gRPC 서비스로, 핵심 역할은 파일(또는 URL)을 수신하고 이를 후속 처리(예: 벡터화)가 가능한 구조화된 텍스트 청크(Chunks)로 파싱하는 것입니다.

+ `server.py`: 서비스의 진입점이자 네트워크 계층입니다. 다중 프로세스, 다중 스레드 gRPC 서버를 시작하여 Go 백엔드의 요청을 수신하고 파싱 결과를 반환합니다.
+ `parser.py`: 디자인 패턴의 **퍼사드(Facade) 패턴**입니다. `Parser` 클래스를 제공하여 내부의 다양한 구체적 파서(PDF, DOCX, Markdown 등)의 복잡성을 숨깁니다. 외부 호출자(`server.py`)는 이 `Parser` 클래스와만 상호 작용하면 됩니다.
+ `base_parser.py`: 파서의 기본 클래스로, 모든 구체적 파서가 공유하는 핵심 로직과 추상 메서드를 정의합니다. 텍스트 분할, 이미지 처리, OCR 및 이미지 설명 생성 등 가장 복잡한 기능을 포함하는 전체 파싱 프로세스의 "두뇌"입니다.

---

### **상세 워크플로우**
Go 백엔드가 비동기 작업을 시작할 때 파일 내용과 구성 정보를 가지고 이 Python 서비스에 gRPC 호출을 시작합니다. 전체 처리 흐름은 다음과 같습니다:

#### **1단계: 요청 수신 및 배포 (**`server.py`** & **`parser.py`**)
1. **gRPC 서비스 진입점 (**`server.py: serve`**)**:
    - 서비스는 `serve()` 함수를 통해 시작됩니다. 환경 변수(`GRPC_WORKER_PROCESSES`, `GRPC_MAX_WORKERS`)에 따라 **다중 프로세스, 다중 스레드** 서버를 시작하여 CPU 자원을 최대한 활용하고 동시 처리 능력을 향상시킵니다.
    - 각 작업 프로세스는 지정된 포트(예: 50051)를 수신하며 요청을 받을 준비를 합니다.
2. **요청 처리 (**`server.py: ReadFromFile`**)**:
    - Go 백엔드가 `ReadFromFile` 요청을 시작하면 작업 프로세스 중 하나가 해당 요청을 수신합니다.
    - 이 메서드는 먼저 요청의 매개변수를 파싱합니다. 여기에는 다음이 포함됩니다:
        * `file_name`, `file_type`, `file_content`: 파일의 기본 정보 및 바이너리 내용.
        * `read_config`: `chunk_size`(청크 크기), `chunk_overlap`(중첩 크기), `enable_multimodal`(멀티모달 처리 활성화 여부), `storage_config`(객체 스토리지 구성), `vlm_config`(시각 언어 모델 구성) 등 모든 파싱 구성을 포함하는 복잡한 객체.
    - 이러한 구성을 `ChunkingConfig` 데이터 객체로 통합합니다.
    - 가장 중요한 단계는 `self.parser.parse_file(...)`을 호출하여 파싱 작업을 `Parser` 퍼사드 클래스에 위임하는 것입니다.
3. **파서 선택 (**`parser.py: Parser.parse_file`**)**:
    - `Parser` 클래스는 작업을 수신한 후 먼저 `get_parser(file_type)` 메서드를 호출합니다.
    - 이 메서드는 파일 유형(예: `'pdf'`)에 따라 딕셔너리 `self.parsers`에서 해당하는 구체적 파서 클래스(예: `PDFParser`)를 찾습니다.
    - 찾으면 해당 `PDFParser` 클래스를 **인스턴스화**하고 `ChunkingConfig` 등 모든 구성 정보를 생성자에 전달합니다.

#### **2단계: 핵심 파싱 및 분할 (**`base_parser.py`**)**
이 단계는 전체 프로세스의 핵심인 **정보의 문맥 무결성 및 원본 순서 보장**을 다룹니다.

`base_parser.py` 코드에 따르면, **최종 분할된 Chunk 내의 텍스트, 표, 이미지는 원본 문서에 나타난 순서대로 저장됩니다**.

이 순서가 보장되는 것은 주로 `BaseParser`의 몇 가지 정교하게 설계된 메서드가 상호 작용하기 때문입니다. 이 과정을 자세히 추적해 보겠습니다.

전체 순서 보장은 세 단계로 나눌 수 있습니다:

1. **1단계: 통합 텍스트 스트림 생성 (**`pdf_parser.py`**)**:
    - `parse_into_text` 메서드에서 코드는 PDF를 **페이지별로** 처리합니다.
    - 각 페이지 내부에서 특정 로직(표가 아닌 텍스트 먼저 추출, 그 다음 표 추가, 마지막으로 이미지 자리 표시자 추가)에 따라 모든 내용을 **하나의 긴 문자열**(`page_content_parts`)로 연결합니다.
    - **핵심 포인트**: 이 단계에서 텍스트, 표, 이미지 자리 표시자의 연결 순서가 문자 수준까지 100% 정확하지 않을 수 있지만, **동일한 페이지의 내용이 함께 있도록** 보장하고 대략적으로 위에서 아래로 읽는 순서를 따릅니다.
    - 마지막으로 모든 페이지의 내용이 `"\n\n--- Page Break ---\n\n"`으로 연결되어 **모든 정보(텍스트, Markdown 표, 이미지 자리 표시자)를 포함하는 단일하고 순서가 지정된 텍스트 스트림 (**`final_text`**)**을 형성합니다.
2. **2단계: 원자화 및 보호 (**`_split_into_units`**)**:
    - 이 단일 `final_text`는 `_split_into_units` 메서드로 전달됩니다.
    - 이 메서드는 **구조적 무결성을 보장하는 핵심**입니다. 정규 표현식을 사용하여 **전체 Markdown 표**와 **전체 Markdown 이미지 자리 표시자**를 **분할할 수 없는 원자 단위(atomic units)**로 식별합니다.
    - 이러한 원자 단위(표, 이미지)와 그 사이의 일반 텍스트 블록을 `final_text`에 나타난 **원본 순서**대로 리스트(`units`)로 분할합니다.
    - **결과**: 이제 우리는 `['일부 텍스트', '![...](...)', '다른 텍스트', '|...|...|\n|---|---|\n...', '더 많은 텍스트']`와 같은 리스트를 갖게 됩니다. 이 리스트의 요소 순서는 **원본 문서의 순서와 완전히 동일합니다**.
3. **3단계: 순차적 분할 (**`chunk_text`**)**:
    - `chunk_text` 메서드는 이 **순서가 지정된 **`units`** 리스트**를 수신합니다.
    - 작동 메커니즘은 매우 간단하고 직접적입니다. 리스트의 각 단위(`unit`)를 **순서대로** 반복합니다.
    - 이 단위들을 임시 `current_chunk` 리스트에 **차례로 추가**하다가 청크의 길이가 `chunk_size` 상한에 가까워지면 중단합니다.
    - 청크가 꽉 차면 저장하고 새로운 청크를 시작합니다(이전 청크의 중첩 부분을 포함할 수 있음).
    - **핵심 포인트**: `chunk_text`가 **`units` 리스트의 순서대로 엄격하게 처리**하기 때문에 표, 텍스트, 이미지 간의 상대적 순서가 절대 섞이지 않습니다. 문서에서 먼저 나타난 표는 반드시 번호가 더 앞선 Chunk에 나타납니다.
4. **4단계: 이미지 정보 첨부 (**`process_chunks_images`**)**:
    - 텍스트 청크가 분할된 후 `process_chunks_images` 메서드가 호출됩니다.
    - 이미 생성된 **각각의** Chunk를 처리합니다.
    - 각 Chunk 내부에서 이미지 자리 표시자를 찾은 다음 AI 처리를 수행합니다.
    - 마지막으로 처리된 이미지 정보(영구 URL, OCR 텍스트, 이미지 설명 등 포함)를 **해당 Chunk 자체**의 `.images` 속성에 첨부합니다.
    - **핵심 포인트**: 이 과정은 **Chunk의 순서나 **`.content`** 내용을 변경하지 않습니다**. 이미 존재하고 순서가 올바른 Chunk에 추가 정보를 첨부할 뿐입니다.

#### **3단계: 멀티모달 처리 (활성화된 경우) (**`base_parser.py`**)**
`enable_multimodal`이 `True`인 경우 텍스트 분할 완료 후 가장 복잡한 멀티모달 처리 단계로 들어갑니다.

1. **동시 작업 시작 (**`BaseParser.process_chunks_images`**)**:
    - 이 메서드는 `asyncio`(Python의 비동기 I/O 프레임워크)를 사용하여 **모든 텍스트 청크 내의 이미지를 동시에 처리**하여 효율성을 크게 높입니다.
    - 각 `Chunk`에 대해 비동기 작업 `process_chunk_images_async`를 생성합니다.
2. **단일 청크 내 이미지 처리 (**`BaseParser.process_chunk_images_async`**)**:
    - **이미지 참조 추출**: 먼저 정규 표현식 `extract_images_from_chunk`를 사용하여 현재 청크의 텍스트에서 모든 이미지 참조(예: `![alt text](image.png)`)를 찾습니다.
    - **이미지 지속성**: 찾은 각 이미지에 대해 동시에 `download_and_upload_image`를 호출합니다. 이 함수는 다음을 담당합니다:
        * 원본 위치(PDF 내부, 로컬 경로 또는 원격 URL일 수 있음)에서 이미지 데이터를 가져옵니다.
        * 이미지를 **구성된 객체 스토리지(COS/MinIO)에 업로드**합니다. 이 단계는 매우 중요하며, 임시적이고 불안정한 이미지 참조를 지속적이고 URL을 통해 공개적으로 액세스 가능한 주소로 변환합니다.
        * 지속성 URL과 이미지 객체(PIL Image)를 반환합니다.
    - **동시 AI 처리**: 성공적으로 업로드된 모든 이미지를 수집하여 `process_multiple_images`를 호출합니다.
        * 이 메서드는 내부적으로 `asyncio.Semaphore`를 사용하여 동시성 수(예: 최대 5개 이미지 동시 처리)를 제한하여 메모리 과다 소모나 모델 API 속도 제한 트리거를 방지합니다.
        * 각 이미지에 대해 `process_image_async`를 호출합니다.
3. **단일 이미지 처리 (**`BaseParser.process_image_async`**)**:
    - **OCR**: `perform_ocr`을 호출하여 OCR 엔진(예: `PaddleOCR`)을 사용하여 이미지 내의 모든 텍스트를 인식합니다.
    - **이미지 설명 (Caption)**: `get_image_caption`을 호출하여 이미지 데이터(Base64로 변환됨)를 구성된 시각 언어 모델(VLM)로 전송하고 이미지 내용에 대한 자연어 설명을 생성합니다.
    - 이 메서드는 `(ocr_text, caption, 지속성 URL)`을 반환합니다.
4. **결과 집계**:
    - 모든 이미지 처리가 완료되면 지속성 URL, OCR 텍스트 및 이미지 설명을 포함하는 구조화된 정보가 해당 `Chunk` 객체의 `.images` 필드에 첨부됩니다.

#### **4단계: 결과 반환 (**`server.py`**)**
1. **데이터 변환 (**`server.py: _convert_chunk_to_proto`**)**:
    - `parser.parse_file` 실행이 완료되면 처리된 모든 `Chunk` 객체를 포함하는 리스트(`ParseResult`)를 반환합니다.
    - `ReadFromFile` 메서드는 이 결과를 수신하고 `_convert_chunk_to_proto`를 호출하여 Python의 `Chunk` 객체(내부 이미지 정보 포함)를 gRPC가 정의한 Protobuf 메시지 형식으로 변환합니다.
2. **응답 반환**:
    - 마지막으로 gRPC 서버는 모든 분할 및 멀티모달 정보를 포함하는 `ReadResponse` 메시지를 호출자인 Go 백엔드 서비스로 다시 전송합니다.

이로써 Go 백엔드는 구조화되고 정보가 풍부한 문서 데이터를 확보하게 되며, 다음 단계인 벡터화 및 인덱스 저장을 진행할 수 있습니다.


## 배포
Docker 이미지 로컬 배포를 지원하며 API 포트를 통해 인터페이스 서비스를 제공합니다.

## 성능 및 모니터링
Weknora는 풍부한 모니터링 및 테스트 구성 요소를 포함합니다:

+ 분산 추적: Jaeger를 통합하여 서비스 아키텍처 내 요청의 전체 실행 경로를 추적합니다. 본질적으로 Jaeger는 사용자가 분산 시스템에서 요청의 전체 수명 주기를 "볼" 수 있도록 돕는 기술입니다.
+ 상태 모니터링: 서비스가 정상 상태인지 모니터링합니다.
+ 확장성: 컨테이너화 배포를 통해 여러 서비스를 통해 대규모 동시 요청을 충족할 수 있습니다.

## QA
### 질문 1: 검색 과정에서 두 번의 하이브리드 검색을 실행하는 목적은 무엇이며, 첫 번째와 두 번째 검색의 차이점은 무엇인가요?
이것은 매우 좋은 관찰입니다. 시스템이 두 번의 하이브리드 검색을 실행하는 것은 **검색의 정확성과 재현율(Recall)을 극대화**하기 위함이며, 본질적으로 **쿼리 확장(Query Expansion)과 다중 전략 검색**의 조합 방법입니다.

#### 목적
두 가지 다른 형태의 쿼리(원본 재작성 문장 vs. 분동된 키워드 시퀀스)를 통해 검색함으로써 시스템은 두 가지 검색 방식의 장점을 결합할 수 있습니다:

+ **의미 검색의 깊이**: 전체 문장을 사용하여 검색하면 벡터 모델(예: `bge-m3`)이 문장의 전체적인 의미를 이해하는 능력을 더 잘 활용하여 의미상 가장 가까운 지식 청크를 찾을 수 있습니다.
+ **키워드 검색의 넓이**: 분동된 키워드를 사용하여 검색하면 지식 청크의 표현 방식이 원래 질문과 다르더라도 핵심 키워드만 포함되어 있다면 검색될 기회를 확보할 수 있습니다. 이는 전통적인 키워드 매칭 알고리즘(예: BM25)에 특히 효과적입니다.

간단히 말해서, **같은 질문을 두 가지 다른 "방식"으로 묻고**, 양쪽의 결과를 취합하여 가장 관련성 높은 지식이 누락되지 않도록 하는 것입니다.

#### 두 검색의 차이점
가장 핵심적인 차이점은 **입력된 쿼리 텍스트(Query Text)**에 있습니다:

1. **첫 번째 하이브리드 검색**
    - **입력**: `rewrite_query` 이벤트 후 생성된, **문법적으로 완전한 자연어 질문**을 사용합니다.
    - **로그 증거**:

```plain
INFO [2025-08-29 09:46:36.896] [request_id=Lkq0OGLYu2fV] knowledgebase.go:266[HybridSearch] | Hybrid search parameters, knowledge base ID: kb-00000001, query text: 需要改写的用户问题是：“入住的房型是什么”。根据提供的信息，入住人Liwx选择的房型是双床房。因此，改写后的完整问题为： “Liwx本次入住的房型是什么”
```

2. **두 번째 하이브리드 검색**
    - **입력**: `preprocess_query` 이벤트 처리 후 생성된, **공백으로 구분된 키워드 시퀀스**를 사용합니다.
    - **로그 증거**:

```plain
INFO [2025-08-29 09:46:37.257] [request_id=Lkq0OGLYu2fV] knowledgebase.go:266[HybridSearch] | Hybrid search parameters, knowledge base ID: kb-00000001, query text: 需要 改写 用户 问题 入住 房型 根据 提供 信息 入住 人 Liwx 选择 房型 双床 房 因此 改写 后 完整 问题 为 Liwx 本次 入住 房型
```

최종적으로 시스템은 이 두 번의 검색 결과를 중복 제거하고 병합하여(로그에 따르면 매번 2개의 결과를 찾았고 중복 제거 후 총 2개임), 후속 답변 생성에 사용할 더 신뢰할 수 있는 지식 집합을 얻습니다.



### 질문 2: 재정렬 모델 분석
Reranker(재정렬기)는 현재 RAG 분야에서 매우 앞선 기술이며, 작동 원리와 적용 시나리오에서 뚜렷한 차이가 있습니다.

간단히 말해서, 이들은 "**전문 판별 모델**"에서 "**대규모 언어 모델(LLM)을 이용한 판별**"로, 그리고 다시 "**LLM 내부 정보를 깊이 파고들어 판별**"하는 방식으로의 진화를 나타냅니다.

다음은 이들의 상세한 차이점입니다:



#### 1. Normal Reranker (일반 재정렬기 / 교차 인코더)
이것은 가장 고전적이고 주류인 재정렬 방법입니다.

+ **모델 유형**: **시퀀스 분류 모델 (Sequence Classification Model)**. 본질적으로 **교차 인코더 (Cross-Encoder)**이며, 일반적으로 BERT, RoBERTa 등 양방향 인코더 아키텍처를 기반으로 합니다. `BAAI/bge-reranker-base/large/v2-m3` 등이 이 범주에 속합니다.
+ **작동 원리**:
    1. **쿼리(Query)**와 **정렬할 문서(Passage)**를 하나의 입력 시퀀스로 연결합니다. 예: `[CLS] what is panda? [SEP] The giant panda is a bear species endemic to China. [SEP]`.
    2. 이 연결된 시퀀스를 모델에 전체적으로 입력합니다. 모델 내부의 셀프 어텐션 메커니즘(Self-Attention)은 쿼리와 문서의 각 단어를 동시에 분석하고 그들 간의 **미세한 상호 작용 관계**를 계산할 수 있습니다.
    3. 모델은 최종적으로 **단일 점수(Logit)**를 출력하며, 이 점수는 쿼리와 문서의 관련성을 직접 나타냅니다. 점수가 높을수록 관련성이 강합니다.
+ **주요 특성**:
    - **장점**: 쿼리와 문서가 모델 내부에서 충분하고 깊이 있게 상호 작용하기 때문에 **정확도가 일반적으로 매우 높으며**, Reranker 성능을 측정하는 황금 표준입니다.
    - **단점**: **속도가 비교적 느립니다**. **모든 "쿼리-문서" 쌍**에 대해 독립적으로 한 번의 완전하고 비용이 많이 드는 계산을 수행해야 하기 때문입니다. 1차 검색에서 100개의 문서를 반환했다면 100번 실행해야 합니다.



#### 2. LLM-based Reranker (LLM 기반 재정렬기)
이 방법은 범용 대규모 언어 모델(LLM)의 능력을 창의적으로 활용하여 재정렬을 수행합니다.

+ **모델 유형**: **인과 언어 모델 (Causal Language Model)**, 즉 우리가 흔히 말하는 GPT, Llama, Gemma와 같은 텍스트 생성용 LLM입니다. `BAAI/bge-reranker-v2-gemma`가 전형적인 예입니다.
+ **작동 원리**:
    1. **점수를 직접 출력하는 것이 아니라**, 재정렬 작업을 **질의응답 또는 텍스트 생성 작업으로 변환**합니다.
    2. 정교하게 설계된 **프롬프트(Prompt)**를 통해 입력을 구성합니다. 예: `"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'. A: {query} B: {passage}"`.
    3. 이 전체 Prompt를 LLM에 제공하고, **LLM이 마지막에 "Yes"라는 단어를 생성할 확률**을 관찰합니다.
    4. 이 **"Yes" 생성 확률(또는 Logit 값)을 관련성 점수로 간주**합니다. 모델이 답이 "Yes"라고 매우 확신한다면 문서 B가 쿼리 A의 답을 포함하고 있다고 생각하는 것, 즉 관련성이 높다는 것을 의미합니다.
+ **주요 특성**:
    - **장점**: LLM의 강력한 **의미 이해, 추론 및 세계 지식**을 활용할 수 있어, 깊은 이해와 추론이 필요한 복잡한 쿼리의 관련성을 판단하는 데 더 효과적일 수 있습니다.
    - **단점**: 계산 비용이 매우 클 수 있으며(LLM 크기에 따라 다름), 성능이 **Prompt 설계에 크게 의존**합니다.



#### 3. LLM-based Layerwise Reranker (LLM 계층 기반 재정렬기)
이것은 두 번째 방법의 "강화판"이며, 더 앞선 복잡한 탐구적 기술입니다.

+ **모델 유형**: 마찬가지로 **인과 언어 모델 (Causal Language Model)**입니다. 예: `BAAI/bge-reranker-v2-minicpm-layerwise`.
+ **작동 원리**:
    1. 입력 부분은 두 번째 방법과 완전히 동일하며 "Yes/No" Prompt를 사용합니다.
    2. 핵심 차이점은 **점수 추출 방식**에 있습니다. 더 이상 LLM의 **마지막 계층** 출력(즉, 최종 예측 결과)에만 의존하지 않습니다.
    3. LLM이 정보를 계층별로 처리하는 과정에서 네트워크 계층(Layer)의 깊이에 따라 서로 다른 수준의 의미 관련성 정보를 포착할 수 있다고 봅니다. 따라서 **모델의 여러 중간 계층**에서 "Yes"라는 단어에 대한 예측 Logit을 추출합니다.
    4. 코드의 `cutoff_layers=[28]` 매개변수는 모델에게 "28번째 계층의 출력을 달라"고 말하는 것입니다. 최종적으로 여러 네트워크 계층에서 온 하나 이상의 점수를 얻게 되며, 이 점수들을 평균하거나 다른 방식으로 조합하여 더 견고한 최종 관련성 판단을 형성합니다.
+ **주요 특성**:
    - **장점**: 이론적으로 **더 풍부하고 포괄적인 관련성 신호**를 얻을 수 있어 마지막 계층만 보는 것보다 더 높은 정확도에 도달할 수 있으며, 현재 성능 한계를 탐구하는 방법 중 하나입니다.
    - **단점**: **복잡도가 가장 높으며**, 중간 계층 정보를 추출하기 위해 모델을 특정하게 수정해야 하고(코드의 `trust_remote_code=True`가 신호임), 계산 비용도 큽니다.

#### 요약 비교
| 특성 | 1. Normal Reranker (일반) | 2. LLM-based Reranker (LLM 기반) | 3. LLM-based Layerwise Reranker (LLM 계층 기반) |
| :--- | :--- | :--- | :--- |
| **기반 모델** | 교차 인코더 (예: BERT) | 인과 언어 모델 (예: Gemma) | 인과 언어 모델 (예: MiniCPM) |
| **작동 원리** | Query와 Passage의 깊은 상호 작용 계산, 관련성 점수 직접 출력 | 정렬 작업을 "Yes/No" 예측으로 변환, "Yes" 확률을 점수로 사용 | 2와 유사하지만 LLM의 여러 중간 계층에서 "Yes" 확률 추출 |
| **출력** | 단일 관련성 점수 | 단일 관련성 점수 (마지막 계층에서) | 여러 관련성 점수 (여러 계층에서) |
| **장점** | **속도와 정확도의 최적 균형**, 성숙하고 안정적 | LLM의 추론 능력 활용, 복잡한 문제 처리 | 이론상 정확도 최고, 신호가 더 풍부함 |
| **단점** | 벡터 검색보다 느림 | 계산 비용 큼, Prompt 설계 의존 | **복잡도 최고**, 계산 비용 최대 |
| **추천 시나리오** | **대부분의 프로덕션 환경에서 최우선 선택**, 효과 좋고 배포 쉬움 | 답변 품질에 대한 요구가 매우 높고 계산 자원이 충분한 경우 | 학술 연구 또는 SOTA(State-of-the-art) 성능 추구 시 |


#### 사용 제안
1. **시작 단계**: **`Normal Reranker`**로 시작하는 것을 강력히 권장합니다. 예: `BAAI/bge-reranker-v2-m3`. 현재 종합 성능이 가장 좋은 모델 중 하나이며, RAG 시스템 성능을 크게 향상시킬 수 있고 통합 및 배포가 비교적 쉽습니다.
2. **심화 탐색**: 일반 Reranker가 미묘하거나 복잡한 추론이 필요한 쿼리를 처리할 때 성능이 좋지 않고 GPU 자원이 충분하다면 `LLM-based Reranker`를 시도해 볼 수 있습니다.
3. **최신 연구**: `Layerwise Reranker`는 연구원이나 특정 작업에서 마지막 성능까지 짜내고 싶은 전문가에게 더 적합합니다.


### 질문 3: 대략적 필터링 또는 정밀 필터링 후의 지식(재정렬 포함)은 어떻게 조립되어 대규모 모델로 전송되나요?
이 부분은 주로 프롬프트 설계와 관련된 전형적인 지침 세부 사항이며, 핵심 임무는 문맥에 따라 사용자 질문에 답변하는 것입니다. 문맥 조립 시 다음을 지정해야 합니다.
주요 제약 사항: 제공된 문서에 따라 엄격하게 답변해야 하며, 자신의 지식을 사용하여 답변하는 것은 금지됩니다.
알 수 없는 상황 처리: 문서에 질문에 답할 충분한 정보가 없는 경우 "보유한 자료로는 이 질문에 답할 수 없습니다"라고 알려주십시오.
인용 요구 사항: 답변 시 특정 문서 내용을 인용한 경우 문장 끝에 문서 번호를 추가하십시오.

---

## 수동 지식 온라인 편집

플랫폼의 지식 베이스 페이지에 "문서 업로드 / 온라인 편집" 이중 진입점이 추가되어 브라우저에서 직접 Markdown 지식을 작성하고 유지 관리할 수 있습니다:

- 초안 모드는 내용을 임시 저장하는 데 사용되며, 초안은 검색에 참여하지 않습니다.
- 게시 작업은 벡터화 및 인덱스 구축을 자동으로 트리거합니다.
- 게시된 Markdown 지식은 다시 열어 편집하고 다시 게시할 수 있습니다.
- 대화 페이지의 어시스턴트 답변 끝에 "지식 베이스에 추가" 도구를 제공하여 현재 문답을 한 번의 클릭으로 편집기로 가져와 확인 후 저장할 수 있습니다.
